{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"962eeb72-0836-4fb1-80cd-154f7d5a111f","cell_type":"markdown","source":"# DQN\n\nМетод обучения DQN — это нейросетевая адаптация алгоритма Q-learning. Также для него разработан набор дополнений, которые становятся актуальными при переходе к обучению глубоких нейронных сетей и решению более сложных задач (то есть задач с бОльшим пространством состояний).\n\nВ этом ноутбуке будет реализован алгоритм DQN для решения среды [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/), цель которой балансировать палочкой в вертикальном положении, управляя только тележкой, к которой она прикреплена. Использована библиотека PyTorch для обучения нейронной сети, аппроксимирующей Q-функцию.\n\nЗа основу взято учебное задание с курса Прикладные задачи анализа данных, майнор Интеллектуальный анализ данных, ФКН НИУ ВШЭ\n\n![cartpole](https://gymnasium.farama.org/_images/cart_pole.gif)\n\n![cartpole](https://www.researchgate.net/publication/362568623/figure/fig5/AS:1187029731807278@1660021350587/Screen-capture-of-the-OpenAI-Gym-CartPole-problem-with-annotations-showing-the-cart.png)","metadata":{"id":"962eeb72-0836-4fb1-80cd-154f7d5a111f"}},{"id":"1c0762ee-f55c-4bec-b544-fb96bcdb5618","cell_type":"code","source":"# If collab, setting dependencies\ntry:\n    import google.colab\n    COLAB = True\nexcept ModuleNotFoundError:\n    COLAB = False\n    pass\n\nif COLAB:\n    !pip -q install \"gymnasium[classic-control, atari, accept-rom-license]\"\n    !pip -q install piglet\n    !pip -q install imageio_ffmpeg\n    !pip -q install moviepy==1.0.3\n    !pip -q install setuptools==59.8.0\n    try:\n        import skmultiflow\n    except ModuleNotFoundError:\n        !git clone --quiet https://github.com/ugadiarov-la-phystech-edu/scikit-multiflow.git && pip -q install ./scikit-multiflow","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c0762ee-f55c-4bec-b544-fb96bcdb5618","outputId":"b0e99477-3ddc-4b78-f460-2d0e0814e96f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: gymnasium 1.1.1 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.8/952.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","arviz 0.21.0 requires setuptools>=60.0.0, but you have setuptools 59.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for scikit-multiflow (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for scikit-multiflow\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scikit-multiflow)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h"]}],"execution_count":1},{"id":"aa4a81a2-b188-47be-8b99-66a95401eed4","cell_type":"code","source":"import abc\nimport base64\nimport io\nimport math\nimport os\nimport random\nimport time\nfrom dataclasses import dataclass\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pygame\nfrom gymnasium import spaces\nfrom gymnasium.envs.registration import WrapperSpec\n%matplotlib inline\n\nif COLAB:\n    from google.colab import files\n    from google.colab.patches import cv2_imshow\n    from google.colab import output","metadata":{"id":"aa4a81a2-b188-47be-8b99-66a95401eed4"},"outputs":[],"execution_count":2},{"id":"24bcf642-10cc-431b-b9b2-18bc2bfbf1a3","cell_type":"markdown","source":"### Action Space\n\nThe action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction of the fixed force the cart is pushed with.\n\n- 0: Push cart to the left\n- 1: Push cart to the right\n\n**Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\nthe pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n\n### Observation Space\n\nThe observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n\n| Num | Observation           | Min                 | Max               |\n|-----|-----------------------|---------------------|-------------------|\n| 0   | Cart Position         | -4.8                | 4.8               |\n| 1   | Cart Velocity         | -Inf                | Inf               |\n| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n| 3   | Pole Angular Velocity | -Inf                | Inf               |\n\n**Note:** While the ranges above denote the possible values for observation space of each element, it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n\n- The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n- The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n   if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n\n### Rewards\n\nSince the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\nincluding the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0.\n\n### Starting State\n\nAll observations are assigned a uniformly random value in `(-0.05, 0.05)`\n\n### Episode End\n\nThe episode ends if any one of the following occurs:\n\n1. Termination: Pole Angle is greater than ±12°\n2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n3. Truncation: Episode length is greater than 500 (200 for v0)","metadata":{"id":"24bcf642-10cc-431b-b9b2-18bc2bfbf1a3"}},{"id":"9d8fea0d-7e29-4eea-888c-26e2bf9f16c4","cell_type":"code","source":"env = gym.make(\"CartPole-v1\", max_episode_steps=1000)\nenv.reset()\n\n# Info about spaces of action and states\nprint(f'{env.observation_space=}')\nprint(f'{env.action_space=}')\n\nn_actions = env.action_space.n\nstate_dim = env.observation_space.shape\nprint(f'Action_space: {n_actions} | State_space: {state_dim}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9d8fea0d-7e29-4eea-888c-26e2bf9f16c4","outputId":"7bdbb348-0d31-445b-e6d6-d15e6068d3dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["env.observation_space=Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n","env.action_space=Discrete(2)\n","Action_space: 2 | State_space: (4,)\n"]}],"execution_count":3},{"id":"febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381","cell_type":"markdown","source":"Т.к. описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n\n- Полносвязные слои (``torch.nn.Linear``)\n- Простые активационные функции (``torch.nn.ReLU``)\n- Сигмоиды и другие похожие функции активации могут плохо работать с ненормализованными входными данными.\n\n- Приближается Q-функция агента, минимизируется среднеквадратичная TD-ошибка:\n$$\n\\delta = Q_{\\theta}(s, a) - [r(s, a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]\n$$\n$$\nL = \\frac{1}{N} \\sum_i \\delta_i^2,\n$$\nгде\n* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние\n* $\\gamma$ дисконтирующий множитель.\n\n$Q_{-}(s',a')$ - это та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети через эти слои не пропускаются градиенты. В научных статьях можно обнаружить следующее обозначение для остановки градиента: $SG(\\cdot)$. В PyTorch есть метод `.detach()` класса `Tensor`, который возвращает тензор с выключенными градиентами, а также контекстный менеджер `with torch.no_grad()`, который задает контекст с вычислениями, для которых не вычисляется градиент.","metadata":{"id":"febc1aff-ab5e-4d9b-9c6d-a6e5b72d2381"}},{"id":"f22be5c6-da87-46aa-95ec-ec792e46a355","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef create_network(input_dim, hidden_dims, output_dim):\n    layers = []\n\n    # 1st layer (input -> hidden)\n    layers.append(nn.Linear(input_dim, hidden_dims[0]))\n    layers.append(nn.ReLU())\n\n    # interlayers (hidden -> hidden)\n    for i in range(len(hidden_dims)-1):\n        layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n        layers.append(nn.ReLU())\n\n    # output layer (hidden -> output)\n    layers.append(nn.Linear(hidden_dims[-1], output_dim))\n\n    # assembling a network\n    network = nn.Sequential(*layers)\n\n    return network","metadata":{"id":"f22be5c6-da87-46aa-95ec-ec792e46a355"},"outputs":[],"execution_count":4},{"id":"25af58f1-46d1-4cf4-abf7-e6f62161595d","cell_type":"markdown","source":"$\\epsilon$-жадный выбор действий:","metadata":{"id":"25af58f1-46d1-4cf4-abf7-e6f62161595d"}},{"id":"d1684ea0-11e4-4f16-a7d3-45c97dbcbc99","cell_type":"code","source":"def select_action_eps_greedy(Q, state, epsilon):\n    \"\"\"\n    Args:\n        Q: neural network predicting Q-values\n        state: current state\n        epsilon: probability of random action selection (0-1)\n    Returns:\n        selected action (int)\n    \"\"\"\n    if not isinstance(state, torch.Tensor):\n        state = torch.tensor(state, dtype=torch.float32)\n\n    # getting Q-values for all actions\n    with torch.no_grad():\n        Q_s = Q(state).numpy()\n\n    # greedy action (max Q-value)\n    greedy_action = np.argmax(Q_s)\n\n    # random action\n    random_action = np.random.randint(len(Q_s))\n\n    # ϵ-greedy selection\n    action = random_action if random.random() < epsilon else greedy_action\n\n    action = int(action)\n    return action\n\n\nQ = create_network(\n    input_dim=np.prod(state_dim), hidden_dims=[64, 64], output_dim=n_actions\n)\nselect_action_eps_greedy(Q, env.reset()[0].flatten(), epsilon=0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1684ea0-11e4-4f16-a7d3-45c97dbcbc99","outputId":"4adf4f8d-a74f-427a-e219-46f83d5093b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":5}],"execution_count":5},{"id":"51cbe381-b616-4359-8ebc-ab8ca63302ef","cell_type":"code","source":"def to_tensor(x, dtype=np.float32):\n    if isinstance(x, torch.Tensor):\n        return x\n    x = np.asarray(x, dtype=dtype)\n    x = torch.from_numpy(x)\n    return x\n\ndef compute_td_target(\n        Q, rewards, next_states, terminateds, gamma=0.99, check_shapes=True,\n):\n    \"\"\"Computes TD-target by formula:\n    target = r + gamma * max(Q(s',a')) * (1 - terminated)\n    \"\"\"\n    # input -> tensors\n    r = to_tensor(rewards)  # shape: [batch_size]\n    s_next = to_tensor(next_states)  # shape: [batch_size, state_size]\n    term = to_tensor(terminateds, bool)  # shape: [batch_size]\n\n    # getting Q[s_next, .] — values of profit of all actions in next state\n    with torch.no_grad():  # turning off gradients for target network\n        Q_sn = Q(s_next)  # shape: [batch_size, n_actions]\n\n    # V^*[s_next] — optimal values of profit in the next state\n    V_sn = torch.max(Q_sn, dim=1)[0]\n\n    # multiply by (1 - terminated): revards -> 0 after termination\n    # counting final TD-target\n    target = r + gamma * V_sn * (~term)\n\n    # checking shapes \n    if check_shapes:\n        assert Q_sn.dim() == 2, \\\n            \"Q_sn must contain q-values for all actions [batch_size, n_actions]\"\n        assert V_sn.dim() == 1, \\\n            \"V_sn must be a vector [batch_size] (max over action axis)\"\n        assert target.dim() == 1, \\\n            \"target must be a vector [batch_size]\"\n\n    return target\n\ndef compute_td_loss(\n        Q, states, actions, td_target, regularizer=.1, out_non_reduced_losses=False\n):\n    \"\"\"Computes TD error (MSE) between current Q-values and TD targets.\n\n    Args:\n        Q: Neural network predicting Q-values\n        states: Current states (batch)\n        actions: Selected actions (batch)\n        td_target: Computed TD targets (batch)\n        regularizer: L1 regularization coefficient\n        out_non_reduced_losses: If True, also returns individual losses\n\n    Returns:\n        Mean loss over the batch (and individual losses if requested)\n    \"\"\"\n    # Convert inputs to tensors\n    s = to_tensor(states)  # shape: [batch_size, state_size]\n    a = to_tensor(actions, int).long()  # shape: [batch_size]\n    td_target = to_tensor(td_target)  # shape: [batch_size]\n\n    # Get Q(s,a) for selected actions\n    Q_s = Q(s)  # shape: [batch_size, n_actions]\n    Q_s_a = Q_s.gather(1, a.unsqueeze(1)).squeeze(1)  # shape: [batch_size]\n\n    # Compute TD error (difference between current estimates and targets)\n    td_error = Q_s_a - td_target  # shape: [batch_size]\n\n    # MSE loss to minimize\n    td_losses = td_error.pow(2)  # shape: [batch_size]\n    loss = td_losses.mean()  # scalar\n\n    # Add L1 regularization on Q-values\n    loss += regularizer * torch.abs(Q_s_a).mean()\n\n    if out_non_reduced_losses:\n        return loss, td_losses.detach()\n    return loss","metadata":{"id":"51cbe381-b616-4359-8ebc-ab8ca63302ef"},"outputs":[],"execution_count":6},{"id":"c66ee2bd-8528-4c72-8c27-20f7aad5286d","cell_type":"code","source":"def eval_dqn(env_name, Q):\n    \"\"\"Evaluates the performance of the algorithm on a single episode\"\"\"\n    env = gym.make(env_name)\n    s, _ = env.reset()\n    done, ep_return = False, 0.\n\n    while not done:\n        # set epsilon = 0 to make an agent act greedy\n        a = select_action_eps_greedy(Q, s, epsilon=0.)\n        s_next, r, terminated, truncated, _ = env.step(a)\n        done = terminated or truncated\n        ep_return += r\n        s = s_next\n\n        if done:\n            break\n\n    return ep_return","metadata":{"id":"c66ee2bd-8528-4c72-8c27-20f7aad5286d"},"outputs":[],"execution_count":7},{"id":"d2be9060-bba6-4d9a-9e4b-0aa50f88839e","cell_type":"code","source":"from collections import deque\n\ndef linear(st, end, duration, t):\n    \"\"\"\n    Linear interpolation of values within the range [st, end],\n    using time progress t relative to the total duration.\n    \"\"\"\n\n    if t >= duration:\n        return end\n    return st + (end - st) * (t / duration)\n\ndef run_dqn(\n        env_name=\"CartPole-v1\",\n        hidden_dims=(128, 128), lr=1e-3, gamma=0.99,\n        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n        train_schedule=1, eval_schedule=1000, smooth_ret_window=10, success_ret=200.\n):\n    env = gym.make(env_name)\n    eval_return_history = deque(maxlen=smooth_ret_window)\n\n    Q = create_network(\n        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n    )\n    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n\n    s, _ = env.reset()\n    done = False\n\n    for global_step in range(1, total_max_steps + 1):\n        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n\n        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n        s_next, r, terminated, truncated, _ = env.step(a)\n        done = terminated or truncated\n\n        if global_step % train_schedule == 0:\n            opt.zero_grad()\n            td_target = compute_td_target(Q, [r], [s_next], [terminated], gamma=gamma)\n            loss = compute_td_loss(Q, [s], [a], td_target)\n            loss.backward()\n            opt.step()\n\n        if global_step % eval_schedule == 0:\n            eval_return = eval_dqn(env_name, Q)\n            eval_return_history.append(eval_return)\n            avg_return = np.mean(eval_return_history)\n            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n            if avg_return >= success_ret:\n                print('Решено!')\n                break\n\n        s = s_next\n        if done:\n            s, _ = env.reset()\n            done = False\n\nrun_dqn(eval_schedule=250)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d2be9060-bba6-4d9a-9e4b-0aa50f88839e","outputId":"18d0a190-3f7c-495c-86cf-16240ce181cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["global_step=250 | avg_return=30.000 | epsilon=0.396\n","global_step=500 | avg_return=20.500 | epsilon=0.392\n","global_step=750 | avg_return=19.667 | epsilon=0.389\n","global_step=1000 | avg_return=18.000 | epsilon=0.385\n","global_step=1250 | avg_return=17.800 | epsilon=0.381\n","global_step=1500 | avg_return=22.667 | epsilon=0.377\n","global_step=1750 | avg_return=23.000 | epsilon=0.373\n","global_step=2000 | avg_return=25.250 | epsilon=0.370\n","global_step=2250 | avg_return=29.667 | epsilon=0.366\n","global_step=2500 | avg_return=29.500 | epsilon=0.362\n","global_step=2750 | avg_return=29.200 | epsilon=0.358\n","global_step=3000 | avg_return=30.800 | epsilon=0.354\n","global_step=3250 | avg_return=36.700 | epsilon=0.351\n","global_step=3500 | avg_return=37.800 | epsilon=0.347\n","global_step=3750 | avg_return=40.400 | epsilon=0.343\n","global_step=4000 | avg_return=38.200 | epsilon=0.339\n","global_step=4250 | avg_return=37.500 | epsilon=0.335\n","global_step=4500 | avg_return=37.500 | epsilon=0.332\n","global_step=4750 | avg_return=34.900 | epsilon=0.328\n","global_step=5000 | avg_return=34.200 | epsilon=0.324\n","global_step=5250 | avg_return=38.900 | epsilon=0.320\n","global_step=5500 | avg_return=47.700 | epsilon=0.316\n","global_step=5750 | avg_return=43.300 | epsilon=0.313\n","global_step=6000 | avg_return=43.900 | epsilon=0.309\n","global_step=6250 | avg_return=42.700 | epsilon=0.305\n","global_step=6500 | avg_return=43.100 | epsilon=0.301\n","global_step=6750 | avg_return=43.200 | epsilon=0.297\n","global_step=7000 | avg_return=47.000 | epsilon=0.294\n","global_step=7250 | avg_return=46.600 | epsilon=0.290\n","global_step=7500 | avg_return=48.300 | epsilon=0.286\n","global_step=7750 | avg_return=45.600 | epsilon=0.282\n","global_step=8000 | avg_return=37.100 | epsilon=0.278\n","global_step=8250 | avg_return=81.000 | epsilon=0.275\n","global_step=8500 | avg_return=80.100 | epsilon=0.271\n","global_step=8750 | avg_return=81.700 | epsilon=0.267\n","global_step=9000 | avg_return=82.400 | epsilon=0.263\n","global_step=9250 | avg_return=84.700 | epsilon=0.259\n","global_step=9500 | avg_return=93.200 | epsilon=0.256\n","global_step=9750 | avg_return=101.900 | epsilon=0.252\n","global_step=10000 | avg_return=102.100 | epsilon=0.248\n","global_step=10250 | avg_return=107.600 | epsilon=0.244\n","global_step=10500 | avg_return=105.700 | epsilon=0.240\n","global_step=10750 | avg_return=108.500 | epsilon=0.237\n","global_step=11000 | avg_return=117.900 | epsilon=0.233\n","global_step=11250 | avg_return=124.200 | epsilon=0.229\n","global_step=11500 | avg_return=136.100 | epsilon=0.225\n","global_step=11750 | avg_return=146.400 | epsilon=0.221\n","global_step=12000 | avg_return=133.500 | epsilon=0.218\n","global_step=12250 | avg_return=171.300 | epsilon=0.214\n","global_step=12500 | avg_return=200.700 | epsilon=0.210\n","Решено!\n"]}],"execution_count":8},{"id":"b6aa1f1e-f346-4081-9aa9-4a1f6655152c","cell_type":"markdown","source":"- `avg_return` - это средняя отдача за эпизод на истории из последних десяти эпизодов. Этот показатель низкий первые 1000 шагов и только затем возрастает и сходится на 5000-15000 шагах (в зависимости от архитектуры сети).\n- Если сеть не достигает нужных результатов к концу цикла, можно увеличить число нейронов в скрытом слое или поменяйте начальный $\\epsilon$.\n- Переменная `epsilon` обеспечивает стремление агента исследовать среду. В данной реализации используется линейное затухание для частоты исследования.","metadata":{"id":"b6aa1f1e-f346-4081-9aa9-4a1f6655152c"}},{"id":"6be1c2a8-37c6-413d-9d57-eff7251943ab","cell_type":"markdown","source":"### DQN with Experience Replay\n\nДобавляется поддержка памяти прецедентов (Replay Buffer), которая будет из себя представлять очередь из наборов: $\\{(s, a, r, s', 1_\\text{terminated})\\}$.\n\nТогда во время обучения каждый новый переход будет добавляться в память, а обучение будет целиком производиться на переходах, просэмплированных из памяти прецедентов.","metadata":{"id":"6be1c2a8-37c6-413d-9d57-eff7251943ab"}},{"id":"a7b5af1a-cd63-4642-9ec2-f1dc71215dff","cell_type":"code","source":"def sample_batch(replay_buffer, n_samples):\n    \"\"\"\n    Randomly samples n_samples transitions from the replay buffer.\n\n    Params:\n    - replay_buffer: a collection of transitions (e.g., deque or list), \n      where each transition is a tuple (state, action, reward, next_state, terminated).\n    - n_samples: number of transitions to sample.\n\n    Output:\n    - states: array of states.\n    - actions: array of actions.\n    - rewards: array of rewards.\n    - next_states: array of next states.\n    - terminateds: array of episode termination flags.\n    \"\"\"\n    rng = np.random.default_rng()\n    indices = rng.choice(len(replay_buffer), size=n_samples, replace=True)\n    samples = [replay_buffer[i] for i in indices]\n\n    # unpack sample to separate lists\n    states, actions, rewards, next_states, terminateds = zip(*samples)\n\n    # lists -> np.array\n    return (\n        np.array(states),\n        np.array(actions),\n        np.array(rewards),\n        np.array(next_states),\n        np.array(terminateds)\n    )","metadata":{"id":"a7b5af1a-cd63-4642-9ec2-f1dc71215dff"},"outputs":[],"execution_count":15},{"id":"76a8db42-6e80-4a38-8449-f13c50ef849c","cell_type":"code","source":"def run_dqn_rb(\n        env_name=\"CartPole-v1\",\n        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n        train_schedule=4, replay_buffer_size=400, batch_size=32,\n        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n):\n    env = gym.make(env_name)\n    replay_buffer = deque(maxlen=replay_buffer_size)\n    eval_return_history = deque(maxlen=smooth_ret_window)\n\n    Q = create_network(\n        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims, output_dim=env.action_space.n\n    )\n    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n\n    s = env.reset()\n    if isinstance(s, tuple):\n        s, _ = s\n    done = False\n\n    for global_step in range(1, total_max_steps + 1):\n        epsilon = linear(eps_st, eps_end, eps_dur * total_max_steps, global_step)\n        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n        s_next, r, terminated, truncated, _ = env.step(a)\n\n        replay_buffer.append((s, a, r, s_next, terminated))\n        done = terminated or truncated\n\n        if global_step % train_schedule == 0:\n            train_batch = sample_batch(replay_buffer, batch_size)\n            states, actions, rewards, next_states, terminateds = train_batch\n\n            opt.zero_grad()\n            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n            loss = compute_td_loss(Q, states, actions, td_target)\n            loss.backward()\n            opt.step()\n\n        if global_step % eval_schedule == 0:\n            eval_return = eval_dqn(env_name, Q)\n            eval_return_history.append(eval_return)\n            avg_return = np.mean(eval_return_history)\n            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n            if avg_return >= success_ret:\n                print('Решено!')\n                break\n\n        s = s_next\n        if done:\n            s, _ = env.reset()\n            done = False\n\nrun_dqn_rb(eval_schedule=250)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76a8db42-6e80-4a38-8449-f13c50ef849c","outputId":"238b4103-334f-47f8-b957-5b6e3e092141"},"outputs":[{"output_type":"stream","name":"stdout","text":["global_step=250 | avg_return=8.000 | epsilon=0.396\n","global_step=500 | avg_return=16.500 | epsilon=0.392\n","global_step=750 | avg_return=18.333 | epsilon=0.389\n","global_step=1000 | avg_return=17.500 | epsilon=0.385\n","global_step=1250 | avg_return=18.000 | epsilon=0.381\n","global_step=1500 | avg_return=35.800 | epsilon=0.377\n","global_step=1750 | avg_return=32.800 | epsilon=0.373\n","global_step=2000 | avg_return=38.600 | epsilon=0.370\n","global_step=2250 | avg_return=58.400 | epsilon=0.366\n","global_step=2500 | avg_return=74.400 | epsilon=0.362\n","global_step=2750 | avg_return=143.400 | epsilon=0.358\n","global_step=3000 | avg_return=153.400 | epsilon=0.354\n","global_step=3250 | avg_return=194.600 | epsilon=0.351\n","global_step=3500 | avg_return=271.800 | epsilon=0.347\n","Решено!\n"]}],"execution_count":16},{"id":"271c3132-f2eb-4d5a-8ba4-8811421ce3cd","cell_type":"markdown","source":"## DQN with Prioritized Experience Replay\n\nКаждому примеру, хранящемуся в памяти, добавляется значение приоритета. Приоритет будет влиять на частоту случайного выбора примеров в пакет на обучение. Удачный выбор приоритета позволит повысить эффективность обучения. Популярным вариантом является абсолютное значение TD-ошибки. Таким образом акцент при обучении Q-функции отводится примерам, на которых аппроксиматор ошибается сильнее.\n\nОднако, нужно помнить, что это значение быстро устаревает, если его не обновлять. Но и обновлять для всей памяти каждый раз накладно. Из-за этого потребуется искать баланс между точностью оценки приоритета и скоростью работы.\n\nВот что я буду делать далее:\n\n- Использовать TD-ошибку в качестве приоритета.\n- Так как для пакета данных, используемых при обучении, в любом случае будет вычислена TD-ошибка, воспользуемся полученными значениями для обновления значений приоритета в памяти для каждого примера из данного пакета.\n- Будем периодически сортировать память для того, чтобы новые добавляемые переходы заменяли собой те переходы, у которых наименьший приоритет (т.е. наименьшие значения ошибки). Сортировка - дорогостоящая операция, поэтому выбрана редкая периодичность.\n\nNB: софтмакс очень чувствителен к масштабу величин и часто требует подбора температуры. Чтобы частично нивелировать эту проблему, можно использовать не `softmax(priorities)` напрямую, а воспользоваться функцией $\\text{symlog} = \\text{sign}(x) \\cdot \\log (|x| + 1)$, то есть `softmax(symlog(priorities))`, и не подбирать температуру. Идея взята из статьи DreamerV2.","metadata":{"id":"271c3132-f2eb-4d5a-8ba4-8811421ce3cd"}},{"id":"3ae2d18b-bb97-4a8f-94ee-259935c039b9","cell_type":"code","source":"def symlog(x):\n    \"\"\"\n    Compute symlog values for a vector `x`.\n    It's an inverse operation for symexp.\n    \"\"\"\n    return np.sign(x) * np.log(np.abs(x) + 1)\n\ndef softmax(xs, temp=1.):\n    exp_xs = np.exp((xs - xs.max()) / temp)\n    return exp_xs / exp_xs.sum()\n\ndef sample_prioritized_batch(replay_buffer, n_samples):\n    # getting priorities\n    priorities = np.array([sample[0] for sample in replay_buffer])\n\n    # transform priorities throught symlog\n    symlog_priorities = symlog(priorities)\n\n    # getting probabilities with softmax\n    probs = softmax(symlog_priorities)\n\n    # selecting indexes by probabilities\n    indices = np.random.choice(len(replay_buffer), size=n_samples, p=probs)\n\n    # getting elements from bufer by indexes\n    sampled = [replay_buffer[idx] for idx in indices]\n\n    states, actions, rewards, next_states, terminateds = zip(*[sample[1:] for sample in sampled])\n\n    batch = (\n        np.array(states), np.array(actions), np.array(rewards),\n        np.array(next_states), np.array(terminateds)\n    )\n    return batch, indices\n\ndef update_batch(replay_buffer, indices, batch, new_priority):\n    \"\"\"Updates batches with corresponding indices\n    replacing their priority values.\"\"\"\n    states, actions, rewards, next_states, terminateds = batch\n\n    for i in range(len(indices)):\n        new_batch = (\n            new_priority[i], states[i], actions[i], rewards[i],\n            next_states[i], terminateds[i]\n        )\n        replay_buffer[indices[i]] = new_batch\n\ndef sort_replay_buffer(replay_buffer):\n    \"\"\"Sorts replay buffer to move samples with\n    lesser priority to the beginning ==> they will be\n    replaced with the new samples sooner.\"\"\"\n    new_rb = deque(maxlen=replay_buffer.maxlen)\n    new_rb.extend(sorted(replay_buffer, key=lambda sample: sample[0]))\n    return new_rb","metadata":{"id":"3ae2d18b-bb97-4a8f-94ee-259935c039b9"},"outputs":[],"execution_count":11},{"id":"88dcccfe-159d-48b3-adcb-d1d5459cdf84","cell_type":"code","source":"import numpy as np\n\nif not hasattr(np, 'bool8'):\n    np.bool8 = np.bool_\n\ndef run_dqn_prioritized_rb(\n        env_name=\"CartPole-v1\",\n        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n        eps_st=.4, eps_end=.02, eps_dur=.25, total_max_steps=100_000,\n        train_schedule=4, replay_buffer_size=400, batch_size=32,\n        eval_schedule=1000, smooth_ret_window=5, success_ret=200.\n):\n    env = gym.make(env_name)\n    replay_buffer = deque(maxlen=replay_buffer_size)\n    eval_return_history = deque(maxlen=smooth_ret_window)\n\n    Q = create_network(\n        input_dim=env.observation_space.shape[0], hidden_dims=hidden_dims,\n        output_dim=env.action_space.n\n    )\n    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n\n    s = env.reset()\n    if isinstance(s, tuple):\n        s, _ = s\n    done = False\n\n    for global_step in range(1, total_max_steps + 1):\n        epsilon = linear(\n            eps_st, eps_end, eps_dur * total_max_steps, global_step\n        )\n        a = select_action_eps_greedy(Q, s, epsilon=epsilon)\n\n        result = env.step(a)\n        if len(result) == 5:\n            s_next, r, terminated, truncated, _ = result\n        else:\n            s_next, r, done, _ = result\n            terminated = done\n            truncated = False\n\n        # Compute new sample loss (compute w/o gradients!)\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(s).unsqueeze(0)  # (1, state_dim)\n            next_state_tensor = torch.FloatTensor(s_next).unsqueeze(0)\n\n            q_values = Q(state_tensor)\n            next_q_values = Q(next_state_tensor)\n\n            # Q(s, a)\n            q_val = q_values[0, a]\n\n            # max_a' Q(s', a')\n            max_next_q_val = next_q_values.max(1).values[0]\n\n            # TD target\n            td_target = r + gamma * max_next_q_val * (1.0 - float(terminated))\n\n            # TD error (scalar)\n            loss = abs(td_target.item() - q_val.item())\n\n        replay_buffer.append((loss, s, a, r, s_next, terminated))\n        done = terminated or truncated\n\n        if global_step % train_schedule == 0:\n            train_batch, indices = sample_prioritized_batch(\n                replay_buffer, batch_size\n            )\n            (\n                states, actions, rewards,\n                next_states, terminateds\n            ) = train_batch\n\n            opt.zero_grad()\n            td_target = compute_td_target(Q, rewards, next_states, terminateds, gamma=gamma)\n            loss, td_losses = compute_td_loss(Q, states, actions, td_target, out_non_reduced_losses=True)\n            loss.backward()\n            opt.step()\n\n            update_batch(\n                replay_buffer, indices, train_batch, td_losses.numpy()\n            )\n\n        # with much slower scheduler periodically re-sort replay buffer\n        # such that it will overwrite the least important samples\n        if global_step % (10 * train_schedule) == 0:\n            replay_buffer = sort_replay_buffer(replay_buffer)\n\n        if global_step % eval_schedule == 0:\n            eval_return = eval_dqn(env_name, Q)\n            eval_return_history.append(eval_return)\n            avg_return = np.mean(eval_return_history)\n            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n            if avg_return >= success_ret:\n                print('Решено!')\n                break\n\n        s = s_next\n        if done:\n            s = env.reset()\n            if isinstance(s, tuple):\n                s, _ = s\n            done = False\n\nrun_dqn_prioritized_rb(eval_schedule=250)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88dcccfe-159d-48b3-adcb-d1d5459cdf84","outputId":"9c80e060-6590-487d-d0f3-bbfcd935154e"},"outputs":[{"output_type":"stream","name":"stdout","text":["global_step=250 | avg_return=16.000 | epsilon=0.396\n","global_step=500 | avg_return=13.000 | epsilon=0.392\n","global_step=750 | avg_return=14.000 | epsilon=0.389\n","global_step=1000 | avg_return=18.500 | epsilon=0.385\n","global_step=1250 | avg_return=18.600 | epsilon=0.381\n","global_step=1500 | avg_return=30.200 | epsilon=0.377\n","global_step=1750 | avg_return=32.800 | epsilon=0.373\n","global_step=2000 | avg_return=35.000 | epsilon=0.370\n","global_step=2250 | avg_return=63.400 | epsilon=0.366\n","global_step=2500 | avg_return=61.800 | epsilon=0.362\n","global_step=2750 | avg_return=88.800 | epsilon=0.358\n","global_step=3000 | avg_return=131.400 | epsilon=0.354\n","global_step=3250 | avg_return=170.400 | epsilon=0.351\n","global_step=3500 | avg_return=186.400 | epsilon=0.347\n","global_step=3750 | avg_return=186.600 | epsilon=0.343\n","global_step=4000 | avg_return=147.000 | epsilon=0.339\n","global_step=4250 | avg_return=143.200 | epsilon=0.335\n","global_step=4500 | avg_return=145.400 | epsilon=0.332\n","global_step=4750 | avg_return=147.400 | epsilon=0.328\n","global_step=5000 | avg_return=202.800 | epsilon=0.324\n","Решено!\n"]}],"execution_count":12}]}